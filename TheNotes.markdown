## An Introduction to Statistical Learning

by Gareth James • Daniela Witten • Trevor Hastie • Robert Tibshirani

### Chapter 1: Introduction

### Chapter 2: Statistical Learning
* pg 17: General form: ![](https://latex.codecogs.com/gif.latex?Y%20%3Df%28X%29&plus;%5Cepsilon), where ![](https://latex.codecogs.com/gif.latex?%5Cepsilon) is a random error term. There are two main reasons that we may wish to estimate ![](https://latex.codecogs.com/gif.latex?f): prediction and inference.
* pg 18: Variability associated with ![](https://latex.codecogs.com/gif.latex?%5Cepsilon) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate ![](https://latex.codecogs.com/gif.latex?f), we cannot reduce the error introduced by ![](https://latex.codecogs.com/gif.latex?%5Cepsilon).
* pg 25: <img src="https://github.com/zhemeng/markdown_attachments/blob/master/Screen%20Shot%202019-09-21%20at%203.48.37%20PM.png" width="700">
The Trade-Off Between Prediction Accuracy and Model Interpretability: as the flexibility of a method increases, its interpretability decreases.
* pg 26: Supervised: For each observation of the predictor measurement(s), there is an associated response measurement, linear regression, logistic regression, GAM, boosting, and support vector machines. Unsupervised: Lack a response vari- able that can supervise our analysis, clustering.
* pg 28: A semi-supervised learning problem. In this setting, we wish to use a statistical learning method that can incorporate the m observations for which response measurements are available as well as the n − m observations for which they are not. We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. Least squares linear regression (Chapter 3) is used with a quantitative response, whereas logistic regression (Chapter 4) is typically used with a qualitative (two-class, or binary) response. As such it is often used as a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K-nearest neighbors (Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of either quantitative or qualitative responses.
* pg 29: In the regression setting, the most commonly-used measure is the mean squared error (MSE): ![](https://latex.codecogs.com/png.latex?MSE%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28y_%7Bi%7D-%5Chat%7Bf%7D%28x_%7Bi%7D%29%29%5E2)
* pg 30: In general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data, we want to choose the method that give the lowest test MSE.
* pg 32: Flexibility, or more for- mally the degrees of freedom, for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve. As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. <img src="https://github.com/zhemeng/markdown_attachments/blob/master/Screen%20Shot%202019-09-21%20at%204.21.20%20PM.png" width="300">
* pg 33: The Bias-Variance Trade-Off: The expected test MSE, for a given value, can always be decomposed into the sum of three fundamental quantities: the variance of fitted f(x0), the squared bias of fitted f(x0) and the variance of the error terms ε. Variance refers to the amount by which fitted f would change if we estimated it using a different training data set. Bias refers to the error that is introduced by approxi- mating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. <img src="https://github.com/zhemeng/markdown_attachments/blob/master/Screen%20Shot%202019-09-21%20at%204.33.07%20PM.png" width="800">
* pg 39: K-nearest neighbors (KNN) classifier. Given a positive integer K and a test observation x0, the KNN classifier first identifies the K points in the training data that are closest to x0, represented by N0. It then estimates the conditional probability for class j as the fraction of points in N0 whose response values equal j, Finally, KNN applies Bayes rule and classifies the test observation x0 to the class with the largest probability.
* pg 40: As 1/K increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10) before increasing again when the method becomes excessively flexible and overfits.

### Chapter 3: Linear Regression
* pg 62: To minimize Residual Sum of Squares: ![](https://latex.codecogs.com/gif.latex?RSS%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7De_i%5E2%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28y_i-%5Chat%7B%5Cbeta%7D_0-%5Chat%7B%5Cbeta%7D_1x_i%29%5E2)
* pg 65: If we use the sample mean μˆ to estimate μ, this estimate is unbiased, on the basis of one particular set of observations y1, . . . , yn, μˆ might overestimate μ, and on the basis of another set of observations, μˆ might underestimate μ. But if we could average a huge number of estimates of μ obtained from a huge number of sets of observations, then this average would exactly equal μ. Hence, an unbiased estimator does not systematically over- or under-estimate the true parameter. How far off will that single estimate of μˆ be? In general, we answer this question by computing the standard error of μˆ, written as SE(μˆ). We have the well-known formula ![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Csigma%20%5E2%7D%7Bn%7D)
* pg 66: The estimate of σ is known as the residual standard error, and is given by the formula ![](https://latex.codecogs.com/gif.latex?RSE%3D%5Csqrt%7BRSS/%28n-2%29%29%7D)
* pg 67: t-statistic: ![](https://latex.codecogs.com/gif.latex?t%3D%5Cfrac%7B%5Chat%7B%5Cbeta%7D_%7B0%7D-0%7D%7BSE%28%5Chat%7B%5Cbeta%7D_%7B0%7D%29%7D), a small p-value indicates that it is unlikely to observe such a substantial association between the pre- dictor and the response due to chance, in the absence of any real association between the predictor and the response.
* pg 68-69: The RSE is an estimate of the standard deviation of ![](https://latex.codecogs.com/gif.latex?%5Cepsilon). It is the average amount that the response will derive from the true regression line. ![](https://latex.codecogs.com/gif.latex?RSE%3D%5Csqrt%7B%5Cfrac%7BRSS%7D%7Bn-2%7D%7D). The RSE is considered a measure of the lack of fit of the model to the data. If the predictions obtained using the model are very close to the true outcome values, then RSE will be small, and we can conclude that the model fits the data very well. On the other hand, if yˆi is very far from yi for one or more observations, then the RSE may be quite large, indicating that the model doesn’t fit the data well.
* pg 70: ![](https://latex.codecogs.com/gif.latex?R%5E2%20%3D%201-%5Cfrac%7BRSS%7D%7BTSS%7D), where ![](https://latex.codecogs.com/gif.latex?TSS%3D%5Csum%20%28y_i-%5Cbar%7By%7D%29%5E2). R2 measures the proportion of variability in Y that can be explained using X. An R2 statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. In the simple linear regression setting, R2 = r2.



* pg 70: When you declare function `foo() { ... }`, you can call the function before its definition. If you use var `foo = function() { ... }`, you can't.
